{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J1iIRbhWT5k2"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import logging\n",
        "from pydantic import BaseModel, ValidationError\n",
        "from typing import List, Literal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TccbZsdhUJJW"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=\"sk-\")\n",
        "openai.api_key = \"sk-\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mj96EA94UM8G"
      },
      "outputs": [],
      "source": [
        "# Pydantic models\n",
        "class LinkedPremiseInput(BaseModel):\n",
        "    premise_text: str\n",
        "    linked_claim_text: str\n",
        "\n",
        "class StanceAnnotatedPremise(BaseModel):\n",
        "    premise_text: str\n",
        "    linked_claim_text: str\n",
        "    stance: Literal[\"pro\", \"con\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CLFOrwCfUWHO"
      },
      "outputs": [],
      "source": [
        "# Main function\n",
        "def classify_stance_with_openai(linked_premises: List[LinkedPremiseInput]) -> List[StanceAnnotatedPremise]:\n",
        "    system_prompt = \"\"\"\n",
        "You are an expert in argument mining. Your task is to determine whether a premise supports (pro) or opposes (con) the linked claim.\n",
        "Respond with one word: \"pro\" or \"con\".\n",
        "\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for item in linked_premises:\n",
        "        premise = item.premise_text\n",
        "        claim = item.linked_claim_text\n",
        "\n",
        "        user_prompt = f\"\"\"Claim: \"{claim}\"\\nPremise: \"{premise}\"\\nWhat is the stance of the premise with respect to the claim?\"\"\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4.1\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "                {\"role\": \"user\", \"content\": user_prompt.strip()}\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "        )\n",
        "\n",
        "        stance_raw = response.choices[0].message.content.strip().lower()\n",
        "        stance = stance_raw if stance_raw in (\"pro\", \"con\") else \"con\"  # default fallback\n",
        "\n",
        "        try:\n",
        "            validated = StanceAnnotatedPremise(\n",
        "                premise_text=premise,\n",
        "                linked_claim_text=claim,\n",
        "                stance=stance\n",
        "            )\n",
        "            results.append(validated)\n",
        "        except ValidationError as e:\n",
        "            print(\"‚ùå Failed to parse or validate OpenAI output.\")\n",
        "            print(\"Raw Output:\\n\", stance_raw)\n",
        "            raise e\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0RVBhC1VtgQ",
        "outputId": "b42bc020-317c-4d73-c93a-4113cb574e08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'premise_text': 'People working remotely often report fewer distractions and better focus.', 'linked_claim_text': 'Remote work improves employee productivity.', 'stance': 'pro'}\n",
            "{'premise_text': 'Most EVs are charged with electricity generated from fossil fuels.', 'linked_claim_text': 'EVs have zero tailpipe emissions.', 'stance': 'con'}\n"
          ]
        }
      ],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "linked_premises = [\n",
        "    LinkedPremiseInput(\n",
        "        premise_text=\"People working remotely often report fewer distractions and better focus.\",\n",
        "        linked_claim_text=\"Remote work improves employee productivity.\"\n",
        "    ),\n",
        "    LinkedPremiseInput(\n",
        "        premise_text=\"Most EVs are charged with electricity generated from fossil fuels.\",\n",
        "        linked_claim_text=\"EVs have zero tailpipe emissions.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "results = classify_stance_with_openai(linked_premises)\n",
        "for r in results:\n",
        "    print(r.model_dump())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Mem8wHYMY476",
        "outputId": "f931a8cc-d8a8-44e7-ee62-614877908d3c"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_7eca24e4-8b92-495c-a448-8dd2797f6d80\", \"output.json\", 381)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "# Save the linked output\n",
        "with open(\"output.json\", \"w\") as f:\n",
        "    json.dump([r.model_dump() for r in results], f, indent=2)\n",
        "\n",
        "# Download to local machine\n",
        "files.download(\"output.json\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
